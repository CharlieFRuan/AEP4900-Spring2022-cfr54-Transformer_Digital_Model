{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPjTnCG-v2Z6",
    "outputId": "553badc6-ed9d-469c-ec74-f8606018ae23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 28 21:55:41 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook for training the embedding model for the Rossler system.\n",
    "Since this is not a built in example, we will need to implement our our config,\n",
    "model and data handler.\n",
    "=====\n",
    "Distributed by: Notre Dame SCAI Lab (MIT Liscense)\n",
    "- Associated publication:\n",
    "url: https://arxiv.org/abs/2010.03957\n",
    "doi: \n",
    "github: https://github.com/zabaras/transformer-physx\n",
    "=====\n",
    "\"\"\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34XMtg9FZFql"
   },
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuqsYQ_2S4kq"
   },
   "source": [
    "Use pip to install from [PyPI](https://pypi.org/project/trphysx/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gvm518_H3AK7",
    "outputId": "1f628ae2-777f-49fc-bb68-ea5848aa284f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trphysx in /usr/local/lib/python3.7/dist-packages (0.0.7)\n",
      "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from trphysx) (1.9.0+cu102)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from trphysx) (3.1.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from trphysx) (3.2.2)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from trphysx) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from trphysx) (1.19.5)\n",
      "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->trphysx) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->trphysx) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->trphysx) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->trphysx) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.0->trphysx) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=3.0.0->trphysx) (1.15.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->trphysx) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install trphysx==0.0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoBoGx0J0LtZ"
   },
   "source": [
    "Mount google drive and create a folder to work in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3K0hSst0b2Ak",
    "outputId": "4cc9aaa7-31fd-43a4-8797-38f214e054d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9zaXL-m8xEx9",
    "outputId": "a9866fa3-5b3e-4471-fc92-5891dd9110b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive\n",
      "/content/gdrive/MyDrive/transformer_physx/rossler\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/\n",
    "% mkdir -p transformer_physx/rossler\n",
    "% cd transformer_physx/rossler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MK_h8wF0Rr4"
   },
   "source": [
    "Now lets download the training and validation data for the lorenz system. Info on wget from [Google drive](https://stackoverflow.com/questions/37453841/download-a-file-from-google-drive-using-wget). This will eventually be update to zenodo repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NtZ02zD0EKo",
    "outputId": "3d9cdf22-4d9a-4fa8-bf3d-a5b16091d30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cU702uo6xIQQ",
    "outputId": "0397214f-9c53-4cf6-929b-32d9882cbdf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-07-28 21:56:22--  https://drive.google.com/uc?export=download&id=1eEXYbiZEz5rlEBoF3erDA_sqNWP0AFtp\n",
      "Resolving drive.google.com (drive.google.com)... 74.125.195.100, 74.125.195.139, 74.125.195.101, ...\n",
      "Connecting to drive.google.com (drive.google.com)|74.125.195.100|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1q307vgb16sfobgmoceq4kb1k577hkuh/1627509375000/01559412990587423567/*/1eEXYbiZEz5rlEBoF3erDA_sqNWP0AFtp?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-07-28 21:56:22--  https://doc-0k-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/1q307vgb16sfobgmoceq4kb1k577hkuh/1627509375000/01559412990587423567/*/1eEXYbiZEz5rlEBoF3erDA_sqNWP0AFtp?e=download\n",
      "Resolving doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
      "Connecting to doc-0k-0o-docs.googleusercontent.com (doc-0k-0o-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [application/x-hdf]\n",
      "Saving to: ‘./data/rossler_training.hdf5’\n",
      "\n",
      "./data/rossler_trai     [ <=>                ]   6.10M  38.4MB/s    in 0.2s    \n",
      "\n",
      "2021-07-28 21:56:22 (38.4 MB/s) - ‘./data/rossler_training.hdf5’ saved [6396672]\n",
      "\n",
      "--2021-07-28 21:56:22--  https://drive.google.com/uc?export=download&id=1LSCmkeM2Z6n8f5bzTkx50YuZvcL2WLsk\n",
      "Resolving drive.google.com (drive.google.com)... 74.125.199.101, 74.125.199.100, 74.125.199.139, ...\n",
      "Connecting to drive.google.com (drive.google.com)|74.125.199.101|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
      "Location: https://doc-00-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7f5if4brgjl84ot45bphuppiurjek0dt/1627509375000/01559412990587423567/*/1LSCmkeM2Z6n8f5bzTkx50YuZvcL2WLsk?e=download [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2021-07-28 21:56:23--  https://doc-00-0o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/7f5if4brgjl84ot45bphuppiurjek0dt/1627509375000/01559412990587423567/*/1LSCmkeM2Z6n8f5bzTkx50YuZvcL2WLsk?e=download\n",
      "Resolving doc-00-0o-docs.googleusercontent.com (doc-00-0o-docs.googleusercontent.com)... 74.125.195.132, 2607:f8b0:400e:c09::84\n",
      "Connecting to doc-00-0o-docs.googleusercontent.com (doc-00-0o-docs.googleusercontent.com)|74.125.195.132|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 799488 (781K) [application/x-hdf]\n",
      "Saving to: ‘./data/rossler_valid.hdf5’\n",
      "\n",
      "./data/rossler_vali 100%[===================>] 780.75K  --.-KB/s    in 0.06s   \n",
      "\n",
      "2021-07-28 21:56:23 (12.7 MB/s) - ‘./data/rossler_valid.hdf5’ saved [799488/799488]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O ./data/rossler_training.hdf5 \"https://drive.google.com/uc?export=download&id=1eEXYbiZEz5rlEBoF3erDA_sqNWP0AFtp\"\n",
    "!wget -O ./data/rossler_valid.hdf5 \"https://drive.google.com/uc?export=download&id=1LSCmkeM2Z6n8f5bzTkx50YuZvcL2WLsk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCHiYnrdZN95"
   },
   "source": [
    "# Transformer-PhysX Rossler System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SoNLzt1xuQk4",
    "outputId": "40e039ef-2c2e-4905-ce41-e5d28f62b10e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2021 21:56:23 - INFO - __main__ -   Torch device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "# Torch imports\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "# Trphysx imports\n",
    "from trphysx.embedding import EmbeddingModel\n",
    "from trphysx.config.configuration_phys import PhysConfig\n",
    "from trphysx.embedding import EmbeddingTrainingHead\n",
    "from trphysx.embedding.training import EmbeddingParser, EmbeddingDataHandler, EmbeddingTrainer\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "TensorTuple = Tuple[torch.Tensor]\n",
    "FloatTuple = Tuple[float]\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "argv = []\n",
    "argv = argv + [\"--exp_name\", \"rossler\"]\n",
    "argv = argv + [\"--training_h5_file\", \"./data/rossler_training.hdf5\"]\n",
    "argv = argv + [\"--eval_h5_file\", \"./data/rossler_valid.hdf5\"]\n",
    "argv = argv + [\"--stride\", \"16\"]\n",
    "argv = argv + [\"--batch_size\", \"256\"]\n",
    "argv = argv + [\"--block_size\", \"16\"]\n",
    "argv = argv + [\"--n_train\", \"1024\"]\n",
    "argv = argv + [\"--n_eval\", \"32\"]\n",
    "argv = argv + [\"--epochs\", \"100\"]\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO)\n",
    "\n",
    "args = EmbeddingParser().parse(argv)    \n",
    "if(torch.cuda.is_available()):\n",
    "    use_cuda = \"cuda\"\n",
    "args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(\"Torch device: {}\".format(args.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYzPn4lLtaNI"
   },
   "source": [
    "## Rossler Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYHwOXUWtcny"
   },
   "outputs": [],
   "source": [
    "class RosslerConfig(PhysConfig):\n",
    "    \"\"\"\n",
    "    This is the configuration class for the modeling of the Rossler system.\n",
    "    \"\"\"\n",
    "\n",
    "    model_type = \"rossler\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_ctx=32,\n",
    "        n_embd=32,\n",
    "        n_layer=4,\n",
    "        n_head=4, # n_head must be a factor of n_embd\n",
    "        state_dims=[3],\n",
    "        activation_function=\"gelu_new\",\n",
    "        initializer_range=0.02,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            n_ctx=n_ctx,\n",
    "            n_embd=n_embd,\n",
    "            n_layer=n_layer,\n",
    "            n_head=n_head,\n",
    "            state_dims=state_dims,\n",
    "            activation_function=activation_function,\n",
    "            initializer_range=initializer_range,\n",
    "            **kwargs\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAefNqpuqaZa"
   },
   "source": [
    "## Embedding Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4CJq3y09rSIQ"
   },
   "outputs": [],
   "source": [
    "class RosslerEmbedding(EmbeddingModel):\n",
    "    \"\"\"Embedding model for the Rossler ODE system\n",
    "\n",
    "    Args:\n",
    "        config (PhysConfig) Configuration class with transformer/embedding parameters\n",
    "    \"\"\"\n",
    "    model_name = \"embedding_rossler\"\n",
    "\n",
    "    def __init__(self, config: PhysConfig) -> None:\n",
    "        \"\"\"Constructor method\n",
    "        \"\"\"\n",
    "        super().__init__(config)\n",
    "\n",
    "        hidden_states = int(abs(config.state_dims[0] - config.n_embd)/2) + 1\n",
    "        hidden_states = 500\n",
    "\n",
    "        self.observableNet = nn.Sequential(\n",
    "            nn.Linear(config.state_dims[0], hidden_states),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_states, config.n_embd),\n",
    "            nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon),\n",
    "            nn.Dropout(config.embd_pdrop)\n",
    "        )\n",
    "\n",
    "        self.recoveryNet = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, hidden_states),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_states, config.state_dims[0])\n",
    "        )\n",
    "        # Learned koopman operator\n",
    "        # Learns skew-symmetric matrix with a diagonal\n",
    "        self.obsdim = config.n_embd\n",
    "        self.kMatrixDiag = nn.Parameter(torch.linspace(1, 0, config.n_embd))\n",
    "\n",
    "        xidx = []\n",
    "        yidx = []\n",
    "        for i in range(1, 3):\n",
    "            yidx.append(np.arange(i, config.n_embd))\n",
    "            xidx.append(np.arange(0, config.n_embd-i))\n",
    "\n",
    "        self.xidx = torch.LongTensor(np.concatenate(xidx))\n",
    "        self.yidx = torch.LongTensor(np.concatenate(yidx))\n",
    "        self.kMatrixUT = nn.Parameter(0.1*torch.rand(self.xidx.size(0)))\n",
    "        # Normalization occurs inside the model\n",
    "        self.register_buffer('mu', torch.tensor([0., 0., 0.]))\n",
    "        self.register_buffer('std', torch.tensor([1., 1., 1.]))\n",
    "        print('Number of embedding parameters: {}'.format( super().num_parameters ))\n",
    "\n",
    "    def forward(self, x: Tensor) -> TensorTuple:\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): [B, 3] Input feature tensor\n",
    "\n",
    "        Returns:\n",
    "            (tuple): tuple containing:\n",
    "\n",
    "                | (torch.Tensor): [B, config.n_embd] Koopman observables\n",
    "                | (torch.Tensor): [B, 3] Recovered feature tensor\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        x = self._normalize(x)\n",
    "        g = self.observableNet(x)\n",
    "        # Decode\n",
    "        out = self.recoveryNet(g)\n",
    "        xhat = self._unnormalize(out)\n",
    "        return g, xhat\n",
    "\n",
    "    def embed(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Embeds tensor of state variables to Koopman observables\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): [B, 3] input feature tensor\n",
    "\n",
    "        Returns:\n",
    "            (Tensor): [B, config.n_embd] Koopman observables\n",
    "        \"\"\"\n",
    "        x = self._normalize(x)\n",
    "        g = self.observableNet(x)\n",
    "        return g\n",
    "\n",
    "    def recover(self, g: Tensor) -> Tensor:\n",
    "        \"\"\"Recovers feature tensor from Koopman observables\n",
    "\n",
    "        Args:\n",
    "            g (Tensor): [B, config.n_embd] Koopman observables\n",
    "\n",
    "        Returns:\n",
    "            (Tensor): [B, 3] Physical feature tensor\n",
    "        \"\"\"\n",
    "        out = self.recoveryNet(g)\n",
    "        x = self._unnormalize(out)\n",
    "        return x\n",
    "\n",
    "    def koopmanOperation(self, g: Tensor) -> Tensor:\n",
    "        \"\"\"Applies the learned koopman operator on the given observables.\n",
    "\n",
    "        Args:\n",
    "            (Tensor): [B, config.n_embd] Koopman observables\n",
    "\n",
    "        Returns:\n",
    "            (Tensor): [B, config.n_embd] Koopman observables at the next time-step\n",
    "        \"\"\"\n",
    "        # Koopman operator\n",
    "        kMatrix = Variable(torch.zeros(self.obsdim, self.obsdim)).to(self.kMatrixUT.device)\n",
    "        # Populate the off diagonal terms\n",
    "        kMatrix[self.xidx, self.yidx] = self.kMatrixUT\n",
    "        kMatrix[self.yidx, self.xidx] = -self.kMatrixUT\n",
    "\n",
    "        # Populate the diagonal\n",
    "        ind = np.diag_indices(kMatrix.shape[0])\n",
    "        kMatrix[ind[0], ind[1]] = self.kMatrixDiag\n",
    "\n",
    "        # Apply Koopman operation\n",
    "        gnext = torch.bmm(kMatrix.expand(g.size(0), kMatrix.size(0), kMatrix.size(0)), g.unsqueeze(-1))\n",
    "        self.kMatrix = kMatrix\n",
    "        return gnext.squeeze(-1) # Squeeze empty dim from bmm\n",
    "\n",
    "    @property\n",
    "    def koopmanOperator(self, requires_grad: bool = True) -> Tensor:\n",
    "        \"\"\"Current Koopman operator\n",
    "\n",
    "        Args:\n",
    "            requires_grad (bool, optional): if to return with gradient storage, defaults to True\n",
    "        \"\"\"\n",
    "        if not requires_grad:\n",
    "            return self.kMatrix.detach()\n",
    "        else:\n",
    "            return self.kMatrix\n",
    "\n",
    "    def _normalize(self, x: Tensor) -> Tensor:\n",
    "        return (x - self.mu.unsqueeze(0))/self.std.unsqueeze(0)\n",
    "\n",
    "    def _unnormalize(self, x: Tensor) -> Tensor:\n",
    "        return self.std.unsqueeze(0)*x + self.mu.unsqueeze(0)\n",
    "\n",
    "    @property\n",
    "    def koopmanDiag(self):\n",
    "        return self.kMatrixDiag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEUXXK7Tt-HG"
   },
   "source": [
    "## Embedding Network Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBUQygCfuC1P"
   },
   "outputs": [],
   "source": [
    "class RosslerEmbeddingTrainer(EmbeddingTrainingHead):\n",
    "    \"\"\"Training head for the Rossler embedding model for parallel training\n",
    "\n",
    "    Args:\n",
    "        config (PhysConfig) Configuration class with transformer/embedding parameters\n",
    "    \"\"\"\n",
    "    def __init__(self, config: PhysConfig) -> None:\n",
    "        \"\"\"Constructor method\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_model = RosslerEmbedding(config)\n",
    "\n",
    "    def forward(self, states: Tensor) -> FloatTuple:\n",
    "        \"\"\"Trains model for a single epoch\n",
    "\n",
    "        Args:\n",
    "            states (Tensor): [B, T, 3] Time-series feature tensor\n",
    "\n",
    "        Returns:\n",
    "            FloatTuple: Tuple containing:\n",
    "            \n",
    "                | (float): Koopman based loss of current epoch\n",
    "                | (float): Reconstruction loss\n",
    "        \"\"\"\n",
    "        self.embedding_model.train()\n",
    "        device = self.embedding_model.devices[0]\n",
    "\n",
    "        loss_reconstruct = 0\n",
    "        mseLoss = nn.MSELoss()\n",
    "\n",
    "        xin0 = states[:,0].to(device) # Time-step\n",
    "\n",
    "        # Model forward for both time-steps\n",
    "        g0, xRec0 = self.embedding_model(xin0)\n",
    "        loss = (1e3)*mseLoss(xin0, xRec0)\n",
    "        loss_reconstruct = loss_reconstruct + mseLoss(xin0, xRec0).detach()\n",
    "\n",
    "        g1_old = g0\n",
    "        # Koopman transform\n",
    "        for t0 in range(1, states.shape[1]):\n",
    "            xin0 = states[:,t0,:].to(device) # Next time-step\n",
    "            _, xRec1 = self.embedding_model(xin0)\n",
    "\n",
    "            g1Pred = self.embedding_model.koopmanOperation(g1_old)\n",
    "            xgRec1 = self.embedding_model.recover(g1Pred)\n",
    "\n",
    "            loss = loss + mseLoss(xgRec1, xin0) + (1e3)*mseLoss(xRec1, xin0) \\\n",
    "                + (1e-1)*torch.sum(torch.pow(self.embedding_model.koopmanOperator, 2))\n",
    "\n",
    "            loss_reconstruct = loss_reconstruct + mseLoss(xRec1, xin0).detach()\n",
    "            g1_old = g1Pred\n",
    "\n",
    "        return loss, loss_reconstruct\n",
    "\n",
    "    def evaluate(self, states: Tensor) -> Tuple[float, Tensor, Tensor]:\n",
    "        \"\"\"Evaluates the embedding models reconstruction error and returns its\n",
    "        predictions.\n",
    "\n",
    "        Args:\n",
    "            states (Tensor): [B, T, 3] Time-series feature tensor\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Float, Tensor, Tensor]: Test error, Predicted states, Target states\n",
    "        \"\"\"\n",
    "        self.embedding_model.eval()\n",
    "        device = self.embedding_model.devices[0]\n",
    "\n",
    "        mseLoss = nn.MSELoss()\n",
    "\n",
    "        # Pull out targets from prediction dataset\n",
    "        yTarget = states[:,1:].to(device)\n",
    "        xInput = states[:,:-1].to(device)\n",
    "        yPred = torch.zeros(yTarget.size()).to(device)\n",
    "\n",
    "        # Test accuracy of one time-step\n",
    "        for i in range(xInput.size(1)):\n",
    "            xInput0 = xInput[:,i].to(device)\n",
    "            g0 = self.embedding_model.embed(xInput0)\n",
    "            yPred0 = self.embedding_model.recover(g0)\n",
    "            yPred[:,i] = yPred0.squeeze().detach()\n",
    "\n",
    "        test_loss = mseLoss(yTarget, yPred)\n",
    "\n",
    "        return test_loss, yPred, yTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtazqCbfu8wC"
   },
   "source": [
    "## Rossler Embedding Data-Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jsamcNLkvFKI"
   },
   "outputs": [],
   "source": [
    "class RosslerDataHandler(EmbeddingDataHandler):\n",
    "    \"\"\"Embedding data handler for Rossler system.\n",
    "    Contains methods for creating training and testing loaders,\n",
    "    dataset class and data collator.\n",
    "    \"\"\"\n",
    "    class RosslerDataset(Dataset):\n",
    "        def __init__(self, examples):\n",
    "            self.examples = examples\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.examples)\n",
    "\n",
    "        def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "            return {\"states\": self.examples[i]}\n",
    "\n",
    "    class RosslerDataCollator:\n",
    "        \"\"\"\n",
    "        Data collator for rossler embedding problem\n",
    "        \"\"\"\n",
    "        # Default collator\n",
    "        def __call__(self, examples:List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "            \n",
    "            x_data_tensor =  torch.stack([example['states'] for example in examples])\n",
    "            return {\"states\": x_data_tensor}\n",
    "\n",
    "    def createTrainingLoader(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        stride:int = 1,\n",
    "        ndata:int = -1,\n",
    "        batch_size:int = 32,\n",
    "        shuffle=True,\n",
    "    ) -> DataLoader:\n",
    "        \"\"\"Creating embedding training data loader for Rossler system.\n",
    "        For a single training simulation, the total time-series is sub-chunked into\n",
    "        smaller blocks for training.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to HDF5 file with training data\n",
    "            block_size (int): The length of time-series blocks\n",
    "            stride (int): Stride of each time-series block\n",
    "            ndata (int, optional): Number of training time-series. If negative, all of the provided \n",
    "            data will be used. Defaults to -1.\n",
    "            batch_size (int, optional): Training batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Turn on mini-batch shuffling in dataloader. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            (DataLoader): Training loader\n",
    "        \"\"\"\n",
    "        logger.info('Creating training loader')\n",
    "        assert os.path.isfile(file_path), \"Training HDF5 file {} not found\".format(file_path)\n",
    "\n",
    "        examples = []\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            # Iterate through stored time-series\n",
    "            samples = 0\n",
    "            for key in f.keys():\n",
    "                data_series = torch.Tensor(f[key])\n",
    "                # Stride over time-series by specified block size\n",
    "                for i in range(0,  data_series.size(0) - block_size + 1, stride): \n",
    "                    examples.append(data_series[i : i + block_size].unsqueeze(0))\n",
    "\n",
    "                samples = samples + 1\n",
    "                if(ndata > 0 and samples > ndata): #If we have enough time-series samples break loop\n",
    "                    break\n",
    "\n",
    "        data = torch.cat(examples, dim=0)\n",
    "        logger.info(\"Training data-set size: {}\".format(data.size()))\n",
    "\n",
    "        # Normalize training data\n",
    "        # Normalize x and y with Gaussian, normalize z with max/min\n",
    "        self.mu = torch.tensor([torch.mean(data[:,:,0]), torch.mean(data[:,:,1]), torch.min(data[:,:,2])])\n",
    "        self.std = torch.tensor([torch.std(data[:,:,0]), torch.std(data[:,:,1]), torch.max(data[:,:,2])-torch.min(data[:,:,2])])\n",
    "\n",
    "        # Needs to min-max normalization due to the reservoir matrix, needing to have a spectral density below 1\n",
    "        if(data.size(0) < batch_size):\n",
    "            logger.warn('Lower batch-size to {:d}'.format(data.size(0)))\n",
    "            batch_size = data.size(0)\n",
    "\n",
    "        dataset = self.RosslerDataset(data)\n",
    "        data_collator = self.RosslerDataCollator()\n",
    "        training_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=data_collator, drop_last=True)\n",
    "        return training_loader\n",
    "\n",
    "    def createTestingLoader(self, \n",
    "        file_path: str,\n",
    "        block_size: int,\n",
    "        ndata:int = -1,\n",
    "        batch_size:int=32,\n",
    "        shuffle=False\n",
    "    ) -> DataLoader:\n",
    "        \"\"\"Creating testing/validation data loader for Rossler system.\n",
    "        For a data case with time-steps [0,T], this method extract a smaller\n",
    "        time-series to be used for testing [0, S], s.t. S < T.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to HDF5 file with testing data\n",
    "            block_size (int): The length of testing time-series\n",
    "            ndata (int, optional): Number of testing time-series. If negative, all of the provided \n",
    "            data will be used. Defaults to -1.\n",
    "            batch_size (int, optional): Testing batch size. Defaults to 32.\n",
    "            shuffle (bool, optional): Turn on mini-batch shuffling in dataloader. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            (DataLoader): Testing/validation data loader\n",
    "        \"\"\"\n",
    "        logger.info('Creating testing loader')\n",
    "        assert os.path.isfile(file_path), \"Testing HDF5 file {} not found\".format(file_path)\n",
    "        \n",
    "        examples = []\n",
    "        with h5py.File(file_path, \"r\") as f:\n",
    "            # Iterate through stored time-series\n",
    "            samples = 0\n",
    "            for key in f.keys():\n",
    "                data_series = torch.Tensor(f[key])\n",
    "                # Stride over time-series\n",
    "                for i in range(0,  data_series.size(0) - block_size + 1, block_size):  # Truncate in block of block_size\n",
    "                    examples.append(data_series[i : i + block_size].unsqueeze(0))\n",
    "                    break\n",
    "\n",
    "                samples = samples + 1\n",
    "                if(ndata > 0 and samples >= ndata): #If we have enough time-series samples break loop\n",
    "                    break\n",
    "\n",
    "        # Combine data-series\n",
    "        data = torch.cat(examples, dim=0)\n",
    "        logger.info(\"Testing data-set size: {}\".format(data.size()))\n",
    "\n",
    "        if(data.size(0) < batch_size):\n",
    "            logger.warn('Lower batch-size to {:d}'.format(data.size(0)))\n",
    "            batch_size = data.size(0)\n",
    "\n",
    "        data = (data - self.mu.unsqueeze(0).unsqueeze(0)) / self.std.unsqueeze(0).unsqueeze(0)\n",
    "        dataset = self.RosslerDataset(data)\n",
    "        data_collator = self.RosslerDataCollator()\n",
    "        testing_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=data_collator, drop_last=False)\n",
    "\n",
    "        return testing_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9Fel9vqZVsH"
   },
   "source": [
    "## Initializing Datasets and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjxiqD3lF98_"
   },
   "source": [
    "Now we can use the auto classes to initialized the predefined configs, dataloaders and models. This may take a bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcpC9Fy243RN",
    "outputId": "d8d5a891-d6a1-4d3d-afa1-13fbcb65d9c5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2021 21:57:05 - INFO - __main__ -   Creating training loader\n",
      "07/28/2021 21:57:20 - INFO - __main__ -   Training data-set size: torch.Size([16384, 16, 3])\n",
      "07/28/2021 21:57:20 - INFO - __main__ -   Creating testing loader\n",
      "07/28/2021 21:57:22 - INFO - __main__ -   Testing data-set size: torch.Size([32, 32, 3])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of embedding parameters: 36192\n"
     ]
    }
   ],
   "source": [
    "data_handler = RosslerDataHandler()\n",
    "# Set up data-loaders\n",
    "training_loader = data_handler.createTrainingLoader(\n",
    "    args.training_h5_file, \n",
    "    block_size=args.block_size, \n",
    "    stride=args.stride, \n",
    "    ndata=args.n_train, \n",
    "    batch_size=args.batch_size)\n",
    "\n",
    "testing_loader = data_handler.createTestingLoader(\n",
    "    args.eval_h5_file, \n",
    "    block_size=32, \n",
    "    ndata=args.n_eval, \n",
    "    batch_size=8)\n",
    "\n",
    "# Load configuration file then init model\n",
    "config = RosslerConfig()\n",
    "model = RosslerEmbeddingTrainer(config)\n",
    "mu, std = data_handler.norm_params\n",
    "model.embedding_model.mu = mu.to(args.device)\n",
    "model.embedding_model.std = std.to(args.device)\n",
    "\n",
    "if args.epoch_start > 1:\n",
    "    model.load_model(args.ckpt_dir, args.epoch_start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auDiMVZ5UfNz"
   },
   "source": [
    "Initialize optimizer and scheduler. Feel free to change if you want to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnrtuKdhGuWQ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr*0.995**(args.epoch_start), weight_decay=1e-8)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It_LLGQIZe0b"
   },
   "source": [
    "## Training the Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2XPKfYTUuXf"
   },
   "source": [
    "Train the model. No visualization here, just boring numbers. This notebook only trains for 100 epochs for brevity, feel free to train longer. The test loss here is only the recovery loss MSE(x - decode(encode(x))) and does not reflect the quality of the Koopman dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ic9DFQcUWpm",
    "outputId": "b8949287-465f-49c7-9c08-5a4ea401c045"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/28/2021 21:57:44 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 1: Training loss 38509596.000, Lr 0.00100\n",
      "07/28/2021 21:57:44 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 1: Test loss: 0.32\n",
      "07/28/2021 21:57:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 2: Training loss 4058091.000, Lr 0.00099\n",
      "07/28/2021 21:57:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 3: Training loss 2628490.500, Lr 0.00099\n",
      "07/28/2021 21:57:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 4: Training loss 1871394.125, Lr 0.00098\n",
      "07/28/2021 21:57:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 5: Training loss 1540042.375, Lr 0.00098\n",
      "07/28/2021 21:57:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 5: Test loss: 0.84\n",
      "07/28/2021 21:57:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 6: Training loss 1334525.625, Lr 0.00097\n",
      "07/28/2021 21:57:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 7: Training loss 1186408.875, Lr 0.00097\n",
      "07/28/2021 21:57:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 8: Training loss 1018499.438, Lr 0.00096\n",
      "07/28/2021 21:58:00 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 9: Training loss 913780.750, Lr 0.00096\n",
      "07/28/2021 21:58:02 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 10: Training loss 797827.938, Lr 0.00095\n",
      "07/28/2021 21:58:02 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 10: Test loss: 0.15\n",
      "07/28/2021 21:58:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 11: Training loss 736426.875, Lr 0.00095\n",
      "07/28/2021 21:58:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 12: Training loss 634202.562, Lr 0.00094\n",
      "07/28/2021 21:58:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 13: Training loss 589102.750, Lr 0.00094\n",
      "07/28/2021 21:58:10 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 14: Training loss 507456.438, Lr 0.00093\n",
      "07/28/2021 21:58:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 15: Training loss 504151.875, Lr 0.00093\n",
      "07/28/2021 21:58:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 15: Test loss: 0.06\n",
      "07/28/2021 21:58:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 16: Training loss 422329.469, Lr 0.00092\n",
      "07/28/2021 21:58:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 17: Training loss 432339.062, Lr 0.00092\n",
      "07/28/2021 21:58:18 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 18: Training loss 390196.688, Lr 0.00091\n",
      "07/28/2021 21:58:20 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 19: Training loss 305929.656, Lr 0.00091\n",
      "07/28/2021 21:58:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 20: Training loss 278885.312, Lr 0.00090\n",
      "07/28/2021 21:58:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 20: Test loss: 0.13\n",
      "07/28/2021 21:58:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 21: Training loss 290824.031, Lr 0.00090\n",
      "07/28/2021 21:58:26 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 22: Training loss 428885.625, Lr 0.00090\n",
      "07/28/2021 21:58:28 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 23: Training loss 340239.625, Lr 0.00089\n",
      "07/28/2021 21:58:31 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 24: Training loss 256899.797, Lr 0.00089\n",
      "07/28/2021 21:58:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 25: Training loss 65524.383, Lr 0.00088\n",
      "07/28/2021 21:58:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 25: Test loss: 0.01\n",
      "07/28/2021 21:58:33 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "07/28/2021 21:58:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 26: Training loss 53992.172, Lr 0.00088\n",
      "07/28/2021 21:58:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 27: Training loss 51515.086, Lr 0.00087\n",
      "07/28/2021 21:58:39 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 28: Training loss 51807.770, Lr 0.00087\n",
      "07/28/2021 21:58:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 29: Training loss 45829.598, Lr 0.00086\n",
      "07/28/2021 21:58:44 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 30: Training loss 48319.047, Lr 0.00086\n",
      "07/28/2021 21:58:44 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 30: Test loss: 0.02\n",
      "07/28/2021 21:58:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 31: Training loss 44409.781, Lr 0.00086\n",
      "07/28/2021 21:58:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 32: Training loss 40493.809, Lr 0.00085\n",
      "07/28/2021 21:58:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 33: Training loss 43316.613, Lr 0.00085\n",
      "07/28/2021 21:58:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 34: Training loss 37401.414, Lr 0.00084\n",
      "07/28/2021 21:58:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 35: Training loss 36344.590, Lr 0.00084\n",
      "07/28/2021 21:58:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 35: Test loss: 0.01\n",
      "07/28/2021 21:58:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 36: Training loss 34578.164, Lr 0.00083\n",
      "07/28/2021 21:58:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 37: Training loss 35332.340, Lr 0.00083\n",
      "07/28/2021 21:59:00 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 38: Training loss 35738.641, Lr 0.00083\n",
      "07/28/2021 21:59:02 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 39: Training loss 30888.936, Lr 0.00082\n",
      "07/28/2021 21:59:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 40: Training loss 82324.406, Lr 0.00082\n",
      "07/28/2021 21:59:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 40: Test loss: 0.00\n",
      "07/28/2021 21:59:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 41: Training loss 248034.469, Lr 0.00081\n",
      "07/28/2021 21:59:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 42: Training loss 205101.000, Lr 0.00081\n",
      "07/28/2021 21:59:10 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 43: Training loss 131497.406, Lr 0.00081\n",
      "07/28/2021 21:59:12 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 44: Training loss 69056.406, Lr 0.00080\n",
      "07/28/2021 21:59:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 45: Training loss 56966.988, Lr 0.00080\n",
      "07/28/2021 21:59:14 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 45: Test loss: 0.00\n",
      "07/28/2021 21:59:16 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 46: Training loss 53391.824, Lr 0.00079\n",
      "07/28/2021 21:59:18 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 47: Training loss 53715.082, Lr 0.00079\n",
      "07/28/2021 21:59:20 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 48: Training loss 47834.957, Lr 0.00079\n",
      "07/28/2021 21:59:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 49: Training loss 46589.207, Lr 0.00078\n",
      "07/28/2021 21:59:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 50: Training loss 44654.520, Lr 0.00078\n",
      "07/28/2021 21:59:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 50: Test loss: 0.00\n",
      "07/28/2021 21:59:24 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "07/28/2021 21:59:27 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 51: Training loss 43911.641, Lr 0.00077\n",
      "07/28/2021 21:59:29 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 52: Training loss 49194.859, Lr 0.00077\n",
      "07/28/2021 21:59:31 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 53: Training loss 130921.188, Lr 0.00077\n",
      "07/28/2021 21:59:33 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 54: Training loss 59318.348, Lr 0.00076\n",
      "07/28/2021 21:59:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 55: Training loss 40745.637, Lr 0.00076\n",
      "07/28/2021 21:59:35 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 55: Test loss: 0.00\n",
      "07/28/2021 21:59:37 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 56: Training loss 13641.349, Lr 0.00076\n",
      "07/28/2021 21:59:39 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 57: Training loss 12560.555, Lr 0.00075\n",
      "07/28/2021 21:59:41 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 58: Training loss 12560.062, Lr 0.00075\n",
      "07/28/2021 21:59:43 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 59: Training loss 13278.677, Lr 0.00074\n",
      "07/28/2021 21:59:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 60: Training loss 65134.805, Lr 0.00074\n",
      "07/28/2021 21:59:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 60: Test loss: 0.00\n",
      "07/28/2021 21:59:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 61: Training loss 28668.416, Lr 0.00074\n",
      "07/28/2021 21:59:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 62: Training loss 25319.082, Lr 0.00073\n",
      "07/28/2021 21:59:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 63: Training loss 59834.383, Lr 0.00073\n",
      "07/28/2021 21:59:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 64: Training loss 20353.775, Lr 0.00073\n",
      "07/28/2021 21:59:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 65: Training loss 12223.723, Lr 0.00072\n",
      "07/28/2021 21:59:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 65: Test loss: 0.00\n",
      "07/28/2021 21:59:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 66: Training loss 10316.092, Lr 0.00072\n",
      "07/28/2021 22:00:00 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 67: Training loss 8950.267, Lr 0.00071\n",
      "07/28/2021 22:00:02 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 68: Training loss 10235.203, Lr 0.00071\n",
      "07/28/2021 22:00:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 69: Training loss 67000.422, Lr 0.00071\n",
      "07/28/2021 22:00:07 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 70: Training loss 43771.984, Lr 0.00070\n",
      "07/28/2021 22:00:07 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 70: Test loss: 0.00\n",
      "07/28/2021 22:00:09 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 71: Training loss 10479.755, Lr 0.00070\n",
      "07/28/2021 22:00:11 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 72: Training loss 8506.892, Lr 0.00070\n",
      "07/28/2021 22:00:13 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 73: Training loss 31535.705, Lr 0.00069\n",
      "07/28/2021 22:00:15 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 74: Training loss 18824.066, Lr 0.00069\n",
      "07/28/2021 22:00:17 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 75: Training loss 15326.374, Lr 0.00069\n",
      "07/28/2021 22:00:17 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 75: Test loss: 0.00\n",
      "07/28/2021 22:00:17 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n",
      "07/28/2021 22:00:19 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 76: Training loss 13526.789, Lr 0.00068\n",
      "07/28/2021 22:00:22 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 77: Training loss 43677.445, Lr 0.00068\n",
      "07/28/2021 22:00:24 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 78: Training loss 49549.891, Lr 0.00068\n",
      "07/28/2021 22:00:26 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 79: Training loss 24407.596, Lr 0.00067\n",
      "07/28/2021 22:00:28 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 80: Training loss 50345.922, Lr 0.00067\n",
      "07/28/2021 22:00:28 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 80: Test loss: 0.00\n",
      "07/28/2021 22:00:30 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 81: Training loss 17724.295, Lr 0.00067\n",
      "07/28/2021 22:00:32 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 82: Training loss 16009.589, Lr 0.00066\n",
      "07/28/2021 22:00:34 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 83: Training loss 15557.715, Lr 0.00066\n",
      "07/28/2021 22:00:36 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 84: Training loss 14543.137, Lr 0.00066\n",
      "07/28/2021 22:00:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 85: Training loss 15712.174, Lr 0.00065\n",
      "07/28/2021 22:00:38 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 85: Test loss: 0.00\n",
      "07/28/2021 22:00:40 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 86: Training loss 14696.871, Lr 0.00065\n",
      "07/28/2021 22:00:42 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 87: Training loss 14250.152, Lr 0.00065\n",
      "07/28/2021 22:00:44 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 88: Training loss 14581.426, Lr 0.00064\n",
      "07/28/2021 22:00:46 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 89: Training loss 8303.272, Lr 0.00064\n",
      "07/28/2021 22:00:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 90: Training loss 5600.450, Lr 0.00064\n",
      "07/28/2021 22:00:48 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 90: Test loss: 0.00\n",
      "07/28/2021 22:00:50 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 91: Training loss 13209.447, Lr 0.00063\n",
      "07/28/2021 22:00:52 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 92: Training loss 17312.537, Lr 0.00063\n",
      "07/28/2021 22:00:54 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 93: Training loss 25934.502, Lr 0.00063\n",
      "07/28/2021 22:00:56 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 94: Training loss 14900.438, Lr 0.00062\n",
      "07/28/2021 22:00:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 95: Training loss 13003.608, Lr 0.00062\n",
      "07/28/2021 22:00:58 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 95: Test loss: 0.00\n",
      "07/28/2021 22:01:00 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 96: Training loss 12451.902, Lr 0.00062\n",
      "07/28/2021 22:01:02 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 97: Training loss 12005.519, Lr 0.00061\n",
      "07/28/2021 22:01:04 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 98: Training loss 11672.692, Lr 0.00061\n",
      "07/28/2021 22:01:06 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 99: Training loss 12141.445, Lr 0.00061\n",
      "07/28/2021 22:01:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 100: Training loss 11681.524, Lr 0.00061\n",
      "07/28/2021 22:01:08 - INFO - trphysx.embedding.training.enn_trainer -   Epoch 100: Test loss: 0.00\n",
      "07/28/2021 22:01:08 - INFO - trphysx.embedding.training.enn_trainer -   Checkpointing model, optimizer and scheduler.\n"
     ]
    }
   ],
   "source": [
    "trainer = EmbeddingTrainer(model, args, (optimizer, scheduler))\n",
    "trainer.train(training_loader, testing_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3ZEE4ixh8nr"
   },
   "source": [
    "Check your Google drive for checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7J5GgEFh_8t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_rossler_enn.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "75e1510848ff81b2a8a3022c3bfac472ed28a49a56e1422a056d525171f2408b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
