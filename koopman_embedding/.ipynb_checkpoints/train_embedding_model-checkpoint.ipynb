{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3677e8e-b9d0-444b-897d-114193dd34ea",
   "metadata": {},
   "source": [
    "Work followed https://github.com/zabaras/transformer-physx and https://arxiv.org/abs/2010.03957."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdb8bcd8-3d5b-44f5-9a57-53a3e729a74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "import time\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Custom types\n",
    "Tensor = torch.Tensor\n",
    "TensorTuple = Tuple[torch.Tensor]\n",
    "FloatTuple = Tuple[float]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52beec-2263-49b5-8432-f1304249e7bd",
   "metadata": {},
   "source": [
    "There are 2 approaches for the Koopman embedding\n",
    "- we either view our system as a 20-variable state, hence using fully-connected embedding network with ReLU activation functions, just like how the author does Lorenz system\n",
    "- or we can view our system as a 2x10 dimension state, hen using convolution network \n",
    "\n",
    "We will first try 20-variable state, we call it FlatKoopmanEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a00abb72-dd34-4cf2-960c-82bb612626fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_Tr = np.zeros(1)\n",
    "theta_Te = np.zeros(1)\n",
    "theta_Tr_batchify = np.zeros(1)\n",
    "theta_Te_batchify = np.zeros(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "path_to_save_model = 'embedding_save_model/'\n",
    "\n",
    "def preprocess_data(NUM_TRAIN, NUM_TEST, batch_size, sanity_check = False):\n",
    "    global theta_Tr, theta_Te, theta_Tr_batchify\n",
    "    NUM_DATA = NUM_TRAIN + NUM_TEST\n",
    "    # 1. Load the data from npz file\n",
    "    if not sanity_check:\n",
    "        data = np.load('../10pendula.npz')\n",
    "    else:\n",
    "        data = np.load('../10pendula_sanity_check.npz')\n",
    "    for key, val in data.items():\n",
    "        exec(key + '=val')\n",
    "    og_theta = data['theta']\n",
    "    # below prints the evolution of the position (angle) over time of the 2nd pendulum in the 121st sample\n",
    "    # plt.plot(og_theta[:, 111, 3, 0])\n",
    "    \n",
    "    # 2. preproceses data to the dimension we want\n",
    "    # now, there are NUM_DATA data entry (i.e. initial conditions), \n",
    "    # each has 60 timesteps, and each timestep is a vector of size 20 \n",
    "    # each feature: [p0, v0, p1, v1, ... p19, v19]\n",
    "    theta = np.transpose(og_theta, (1, 0, 2, 3)).reshape(50000, 60, 20)\n",
    "    theta = theta[:NUM_DATA, :, :]\n",
    "    N, _, _ = theta.shape\n",
    "    cutoff = int(np.ceil(0.7 * N))\n",
    "    theta_Tr = theta[:cutoff, :, :]\n",
    "    theta_Te = theta[cutoff: , :, :]\n",
    "    \n",
    "    # 3. batchify theta_Tr and theta_Te\n",
    "    batch_size = theta_Tr.shape[0] / batch_num\n",
    "    theta_Tr_batchify = \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494aa04-e63d-4373-a774-2c1b8770c418",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Koopman Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "314e70e8-bbc0-46d8-9503-51b695e5fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatKoopmanEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    the model following original paper's Lorenz System's model, where input is a flat 20x1 vector.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        hidden_states = config['hidden_states']\n",
    "        \n",
    "        self.observableNet = nn.Sequential(\n",
    "            nn.Linear(config['state_dims'][0], hidden_states),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_states, config['n_embd']),\n",
    "            nn.LayerNorm(config['n_embd'], eps=config['layer_norm_epsilon']),\n",
    "            nn.Dropout(config['embd_pdrop'])\n",
    "        )\n",
    "        \n",
    "        self.recoveryNet = nn.Sequential(\n",
    "            nn.Linear(config['n_embd'], hidden_states),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_states, config['state_dims'][0])\n",
    "        )\n",
    "        \n",
    "        # Learned Koopman operator\n",
    "        self.obsdim = config['n_embd']\n",
    "        self.kMatrixDiag = nn.Parameter(torch.linspace(1, 0, config['n_embd']))\n",
    "        \n",
    "        # Off-diagonal indices\n",
    "        xidx = []\n",
    "        yidx = []\n",
    "        for i in range(1, 3):\n",
    "            yidx.append(np.arange(i, config['n_embd']))\n",
    "            xidx.append(np.arange(0, config['n_embd']))\n",
    "        \n",
    "        self.xidx = torch.LongTensor(np.concatenate(xidx))\n",
    "        self.yidx = torch.LongTensor(np.concatenate(yidx))\n",
    "        self.kMatrixUT = nn.Parameter(0.1*torch.rand(self.xidx.size(0)))\n",
    "        \n",
    "        # Normalization occurs inside the model\n",
    "        self.xidx = torch.LongTensor(np.concatenate(xidx))\n",
    "        self.yidx = torch.LongTensor(np.concatenate(yidx))\n",
    "        self.kMatrixUT = nn.Parameter(0.1*torch.rand(self.xidx.size(0)))\n",
    "        \n",
    "    def forward(self, x: Tensor) -> TensorTuple:\n",
    "        \"\"\"Forward pass\n",
    "        Args:\n",
    "            x (Tensor): [B, config['state_dims'][0]] Input feature tensor\n",
    "        Returns:\n",
    "            TensorTuple: Tuple containing:\n",
    "                | (Tensor): [B, config.n_embd] Koopman observables\n",
    "                | (Tensor): [B, config['state_dims'][0]] Recovered feature tensor\n",
    "        \"\"\"\n",
    "        # note that we just applied f and g, there is no K operator invovled here\n",
    "        # Encode\n",
    "        x = self._normalize(x)\n",
    "        g = self.observableNet(x)\n",
    "        # Decode\n",
    "        out = self.recoveryNet(g) \n",
    "        xhat = self._unnormalize(out)\n",
    "        return g, xhat\n",
    "    \n",
    "    def embed(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Embeds tensor of state variables to Koopman observables\n",
    "        Args:\n",
    "            x (Tensor): [B, config['state_dims'][0]] Input feature tensor\n",
    "        Returns:\n",
    "            Tensor: [B, config.n_embd] Koopman observables\n",
    "        \"\"\"\n",
    "        x = self._normalize(x)\n",
    "        g = self.observableNet(x)\n",
    "        return g\n",
    "        \n",
    "    def recover(self, g: Tensor) -> Tensor:\n",
    "        \"\"\"Recovers feature tensor from Koopman observables\n",
    "        Args:\n",
    "            g (Tensor): [B, config.n_embd] Koopman observables\n",
    "        Returns:\n",
    "            Tensor: [B, config['state_dims'][0]] Physical feature tensor\n",
    "        \"\"\"\n",
    "        out = self.recoveryNet(g)\n",
    "        x = self._unnormalize(out)\n",
    "        return x\n",
    "    \n",
    "    def koopmanOperation(self, g: Tensor) -> Tensor:\n",
    "        \"\"\"Applies the learned Koopman operator on the given observables\n",
    "        Args:\n",
    "            g (Tensor): [B, config.n_embd] Koopman observables\n",
    "        Returns:\n",
    "            (Tensor): [B, config.n_embd] Koopman observables at the next time-step\n",
    "        \"\"\"\n",
    "        # Koopman operator\n",
    "        kMatrix = Variable(torch.zeros(self.obsdim, self.obsdim)).to(self.kMatrixUT.device)\n",
    "        # Populate the off diagonal terms\n",
    "        kMatrix[self.xidx, self.yidx] = self.kMatrixUT\n",
    "        kMatrix[self.yidx, self.xidx] = -self.kMatrixUT\n",
    "\n",
    "        # Populate the diagonal\n",
    "        ind = np.diag_indices(kMatrix.shape[0])\n",
    "        kMatrix[ind[0], ind[1]] = self.kMatrixDiag\n",
    "\n",
    "        # Apply Koopman operation\n",
    "        gnext = torch.bmm(kMatrix.expand(g.size(0), kMatrix.size(0), kMatrix.size(0)), g.unsqueeze(-1))\n",
    "        self.kMatrix = kMatrix\n",
    "        return gnext.squeeze(-1) # Squeeze empty dim from bmm\n",
    "    \n",
    "    @property\n",
    "    def koopmanOperator(self, requires_grad: bool =True) -> Tensor:\n",
    "        \"\"\"Current Koopman operator\n",
    "        Args:\n",
    "            requires_grad (bool, optional): If to return with gradient storage. Defaults to True\n",
    "        Returns:\n",
    "            (Tensor): Full Koopman operator tensor\n",
    "        \"\"\"\n",
    "        if not requires_grad:\n",
    "            return self.kMatrix.detach()\n",
    "        else:\n",
    "            return self.kMatrix\n",
    "    \n",
    "    def _normalize(self, x):\n",
    "        return (x - self.mu.unsqueeze(0))/self.std.unsqueeze(0)\n",
    "\n",
    "    def _unnormalize(self, x):\n",
    "        return self.std.unsqueeze(0)*x + self.mu.unsqueeze(0)\n",
    "        \n",
    "    @property\n",
    "    def koopmanDiag(self):\n",
    "        return self.kMatrixDiag        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd67f9e5-4772-457a-8378-0bb7b41f8f70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Koopman Embedding Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80ed2443-5a76-4073-bc42-f9bfc65c005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlatKoopmanEmbeddingTrainer(nn.Module):\n",
    "    \"\"\"\n",
    "    Training head for the Lorenz embedding model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Constructor method\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding_model = FlatKoopmanEmbedding()\n",
    "    \n",
    "    def forward(self, states: Tensor) -> FloatTuple:\n",
    "        \"\"\"\n",
    "        Trains model for a single epoch\n",
    "        Args:\n",
    "            states (Tensor): [B, T, config['state_dims'][0]] Time-series feature tensor\n",
    "        Returns:\n",
    "            FloatTuple: Tuple containing:\n",
    "            \n",
    "                | (float): Koopman based loss of current epoch (to check whether predicted correctly)\n",
    "                | (float): Reconstruction loss (to check whether g and f are inverse)\n",
    "        \"\"\"\n",
    "        self.embedding_model.train()\n",
    "        \n",
    "        loss_reconstruct = 0\n",
    "        mseLoss = nn.MSELoss()\n",
    "        \n",
    "        xin0 = states[:,0].to(device) # Time-step - the first time step's states for each batch\n",
    "        \n",
    "        # Model forward for initial time-step\n",
    "        g0, xRec0 = self.embedding_model(xin0)\n",
    "        loss = (1e4)*mseLoss(xin0, xRec0)\n",
    "        loss_reconstruct = loss_reconstruct + mseLoss(xin0, xRec0).detach() \n",
    "        \n",
    "        g1_old = g0\n",
    "        # Loop through time-series\n",
    "        for t0 in range(1, states.shape[1]):\n",
    "            xin0 = states[:,t0,:].to(device) # Next time-step\n",
    "            _, xRec1 = self.embedding_model(xin0) # xRec is xin0 applied with f and g\n",
    "\n",
    "            g1Pred = self.embedding_model.koopmanOperation(g1_old) # apply K operator\n",
    "            xgRec1 = self.embedding_model.recover(g1Pred) # apply g on after K operator\n",
    "            \n",
    "            # like the paper says, there are 3 types of loss; note that the coeff here \n",
    "            # are from the original paper, might be magic number\n",
    "            loss = loss + mseLoss(xgRec1, xin0) + (1e4)*mseLoss(xRec1, xin0) \\\n",
    "                + (1e-1)*torch.sum(torch.pow(self.embedding_model.koopmanOperator, 2))\n",
    "\n",
    "            loss_reconstruct = loss_reconstruct + mseLoss(xRec1, xin0).detach()\n",
    "            g1_old = g1Pred\n",
    "\n",
    "        return loss, loss_reconstruct\n",
    "    \n",
    "    def evaluate(self, states: Tensor) -> Tuple[float, Tensor, Tensor]:\n",
    "        \"\"\"Evaluates the embedding models reconstruction error and returns its\n",
    "        predictions.\n",
    "        Args:\n",
    "            states (Tensor): [B, T, 3] Time-series feature tensor\n",
    "        Returns:\n",
    "            Tuple[Float, Tensor, Tensor]: Test error, Predicted states, Target states\n",
    "        \"\"\"\n",
    "        self.embedding_model.eval()\n",
    "\n",
    "        mseLoss = nn.MSELoss()\n",
    "\n",
    "        # Pull out targets from prediction dataset\n",
    "        yTarget = states[:,1:].to(device)\n",
    "        xInput = states[:,:-1].to(device)\n",
    "        yPred = torch.zeros(yTarget.size()).to(device)\n",
    "\n",
    "        # Test accuracy of one time-step\n",
    "        for i in range(xInput.size(1)):\n",
    "            # note that we are ONLY looking at the reconstruction error\n",
    "            xInput0 = xInput[:,i].to(device)\n",
    "            g0 = self.embedding_model.embed(xInput0) \n",
    "            yPred0 = self.embedding_model.recover(g0)\n",
    "            yPred[:,i] = yPred0.squeeze().detach()\n",
    "\n",
    "        test_loss = mseLoss(yTarget, yPred)\n",
    "\n",
    "        return test_loss, yPred, yTarget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6971a2a0-5e91-4cb7-b3c8-7706b1f402e6",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982e475-fa29-41de-9cfc-750703df4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingTrainer:\n",
    "    \"\"\"Trainer for Koopman embedding model\n",
    "    Args:\n",
    "        model (EmbeddingTrainingHead): Embedding training model\n",
    "        args (TrainingArguments): Training arguments\n",
    "        optimizers (Tuple[Optimizer, Scheduler]): Tuple of Pytorch optimizer and lr scheduler.\n",
    "        viz (Viz, optional): Visualization class. Defaults to None.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, args,\n",
    "        optimizers: Tuple[Optimizer, Scheduler]) -> None:\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)\n",
    "        self.args = args\n",
    "        self.optimizers = optimizers\n",
    "        \n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Training loop for the embedding model\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizers[0]\n",
    "        lr_scheduler = self.optimizers[1]\n",
    "        # Loop over epochs\n",
    "        for epoch in range(1, self.args[epochs] + 1):\n",
    "            loss_total = 0.0\n",
    "            loss_reconstruct = 0.0\n",
    "            self.model.zero_grad()\n",
    "            for mbidx, inputs in enumerate(training_loader):\n",
    "\n",
    "                loss0, loss_reconstruct0 = self.model(**inputs)\n",
    "                loss0 = loss0.sum()\n",
    "\n",
    "                loss_reconstruct = loss_reconstruct + loss_reconstruct0.sum()\n",
    "                loss_total = loss_total + loss0.detach()\n",
    "                # Backwards!\n",
    "                loss0.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.1)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if mbidx+1 % 10 == 0:\n",
    "                    logger.info('Epoch {:d}: Completed mini-batch {}/{}.'.format(epoch, mbidx+1, len(training_loader)))\n",
    "\n",
    "            # Progress learning rate scheduler\n",
    "            lr_scheduler.step()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                cur_lr = param_group['lr']\n",
    "                break\n",
    "            logger.info(\"Epoch {:d}: Training loss {:.03f}, Lr {:.05f}\".format(epoch, loss_total, cur_lr))\n",
    "\n",
    "            # Evaluate current model\n",
    "            if(epoch%5 == 0 or epoch == 1):\n",
    "                output = self.evaluate(eval_dataloader, epoch=epoch)\n",
    "                logger.info('Epoch {:d}: Test loss: {:.02f}'.format(epoch, output['test_error']))\n",
    "\n",
    "            # Save model checkpoint\n",
    "            if epoch % self.args.save_steps == 0:\n",
    "                logger.info(\"Checkpointing model, optimizer and scheduler.\")\n",
    "                # Save model checkpoint\n",
    "                self.model.save_model(self.args.ckpt_dir, epoch=epoch)\n",
    "                torch.save(optimizer.state_dict(), os.path.join(self.args.ckpt_dir, \"optimizer{:d}.pt\".format(epoch)))\n",
    "                torch.save(lr_scheduler.state_dict(), os.path.join(self.args.ckpt_dir, \"scheduler{:d}.pt\".format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7b565-ee57-45a9-aec0-1194947daccb",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae662e78-c46d-400b-99df-ba7beab54528",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'hidden_states': 500, \n",
    "          'state_dims': [20], # since we view it as a 20x1 vector\n",
    "          'n_embd': 64, \n",
    "          'layer_norm_epsilon': 1e-5, \n",
    "          'embd_pdrop': 0.0,\n",
    "          'epochs': 300\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3431ec6e-aaba-4a6a-a946-9cc1c3d589a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_data(NUM_DATA = 300, sanity_check = False)\n",
    "theta_Tr = torch.tensor(theta_Tr)\n",
    "model = FlatKoopmanEmbedding(config)\n",
    "model.mu = torch.tensor([torch.mean(theta_Tr[:,:,0]) for i in range(theta_Tr.shape[2])])\n",
    "model.std = torch.tensor([torch.std(theta_Tr[:,:,0]) for i in range(theta_Tr.shape[2])])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args.lr*0.995**(args.epoch_start-1), weight_decay=1e-8)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33b266f1-41f7-42fe-9aed-2461a4d6ef4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes 1 positional argument but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fb/_j_dwd_s4bs08b9lf5qjhmy40000gn/T/ipykernel_23621/3092219907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatKoopmanEmbeddingTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() takes 1 positional argument but 4 were given"
     ]
    }
   ],
   "source": [
    "trainer = FlatKoopmanEmbeddingTrainer(model, config, (optimizer, scheduler))\n",
    "trainer.train(training_loader, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38d3da0-cbcd-4171-b975-f0f1435a855c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
